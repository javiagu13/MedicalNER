{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "medNer-SemanticType-version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtXs8Ygnbpch",
        "outputId": "392c2060-ace1-44bd-e9cd-fa0bac550fd7"
      },
      "source": [
        "#!pip install --upgrade google-api-python-client\n",
        "#!pip install --user google-colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhMZw10lLvmS"
      },
      "source": [
        "###IMPORTANT-> DO NOT RUN THIS PART UNLESS YOU WANT TO CREATE NEW TRAINING TEST AND DEV DATASETS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7fcxPapDf1e"
      },
      "source": [
        "### CREATING TEST, TRAIN AND DEV FROM MEDMENTIONS CORPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipuAjaI5CZ1C"
      },
      "source": [
        "READING FILES: dev, test, train and corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8i4hMnkb6U1"
      },
      "source": [
        "#read file DEV\n",
        "devTxt=\"/content/drive/MyDrive/medNer/corpus_pubtator_pmids_dev.txt\"\n",
        "with open(devTxt, \"r\") as file1:\n",
        "    devTxtList = file1.readlines()\n",
        "#delete \\n at the end of every line    \n",
        "for i in range(0,len(devTxtList)):\n",
        "  devTxtList[i]=devTxtList[i].split('\\n')[0]\n",
        "\n",
        "\n",
        "#read file TEST\n",
        "testTxt=\"/content/drive/MyDrive/medNer/corpus_pubtator_pmids_test.txt\"\n",
        "with open(testTxt, \"r\") as file2:\n",
        "    testTxtList = file2.readlines()\n",
        "#delete \\n at the end of every line    \n",
        "for i in range(0,len(testTxtList)):\n",
        "  testTxtList[i]=testTxtList[i].split('\\n')[0]\n",
        "\n",
        "\n",
        "#read file TRAIN\n",
        "trngTxt=\"/content/drive/MyDrive/medNer/corpus_pubtator_pmids_trng.txt\"\n",
        "with open(trngTxt, \"r\") as file3:\n",
        "    trngTxtList = file3.readlines()\n",
        "#delete \\n at the end of every line    \n",
        "for i in range(0,len(trngTxtList)):\n",
        "  trngTxtList[i]=trngTxtList[i].split('\\n')[0]\n",
        "\n",
        "\n",
        "#read file CORPUS\n",
        "corpusTxt=\"/content/drive/MyDrive/medNer/corpus_pubtator.txt\"\n",
        "with open(corpusTxt, \"r\") as file4:\n",
        "    corpusTxtList = file4.readlines()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inr-XM4KmEzd"
      },
      "source": [
        "LOADING TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqFT1nkIWjV"
      },
      "source": [
        "#DE PRUEBAS\n",
        "# http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py -en bertsio motza\n",
        "import nltk, re, json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "# Ordena garrantzitsua da  (lehenetik azkenera egiten dire parekatzeak)\n",
        "\n",
        "# Erabiltzaile-izenak batera mantendu (@ ikurraz hasitako edozein token, eta ondoren A-Z, a-z, 0-9)\n",
        "regexes=(\n",
        "# zuriunea ez den beste edozer\n",
        "r\"(?:\\w+\\.\\w+)\", \n",
        "#symbols\n",
        "r\"(?:[\\<\\>\\=\\+|\\-|\\/|\\*\\\\\\'\\\"|\\%|\\w]+)\",   \n",
        "\n",
        "# hashtag-ak elkarrekin mantendu (# ikurraz hasitako edozein token, eta ondoren A-Z, a-z, 0-9, _, edo -)\n",
        "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "big_regex=\"|\".join(regexes)\n",
        "\n",
        "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "def my_extensible_tokenize(text):\n",
        "    return my_extensible_tokenizer.findall(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvK-wFZIaTl9"
      },
      "source": [
        "import nltk, re, json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "regexes=(\n",
        "r\"(?:T[0-9]+)\", \n",
        ")\n",
        "\n",
        "big_regex=\"|\".join(regexes)\n",
        "\n",
        "semanticTypeTokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "def semanticTypeTokenizerFunc(text):\n",
        "    return semanticTypeTokenizer.findall(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0jKf0RCxDu"
      },
      "source": [
        "CREATING A FUNCTION TO NORMALIZE ANY STRING USING THE TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1MdXkSXLGtT"
      },
      "source": [
        "def normalizeString(string):\n",
        "  #print(string)\n",
        "  last_char = string[-1]\n",
        "  isEndOrComma=None\n",
        "\n",
        "  if last_char == '.':\n",
        "    isEndOrComma=\"END\"\n",
        "  if last_char == ',':\n",
        "    isEndOrComma=\"COMMA\"\n",
        "    \n",
        "  string=my_extensible_tokenize(string)\n",
        "  #print(string)\n",
        "  if len(string)==0:\n",
        "    return \"\", isEndOrComma\n",
        "  else:\n",
        "    return string[0], isEndOrComma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hgG1mKFId0I",
        "outputId": "810666c3-8ee7-4e2d-ae08-4abe14919944"
      },
      "source": [
        "normalizeString(\"(hola,)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hola', None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20K8AEhXaV2T"
      },
      "source": [
        "def normalizeSemanticType(string):   \n",
        "  lista=semanticTypeTokenizerFunc(string) \n",
        "  if len(lista)==0:\n",
        "    return \"\"\n",
        "  return lista[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G526cglBeMjd"
      },
      "source": [
        "GETTING ORDER MOST APPEARING OF SEMANTIC TYPES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yV-YAyTeMHj",
        "outputId": "a66bccb3-4f84-4873-d360-16dd7fc05ada"
      },
      "source": [
        "from collections import Counter\n",
        "import collections\n",
        "import operator\n",
        "inPaper=False\n",
        "titleRow=0\n",
        "paperRow=1\n",
        "SematicTypeAppearence=[]\n",
        "\n",
        "#COMMENT NEXT TWO LINES FOR WHOLE CORPUS\n",
        "#maxAmountOfPapers=100\n",
        "paperKont=0\n",
        "\n",
        "with open('/content/drive/MyDrive/medNer/medMentions-train-SemType-version.tsv', 'r') as read_file:\n",
        "\n",
        "  for i in range(0,len(corpusTxtList)):\n",
        "    \n",
        "    #beginning of paper\n",
        "    if(inPaper==False):\n",
        "      #number of paper\n",
        "      paperNumber=corpusTxtList[i+2].split('\\t')[0]\n",
        "      #title\n",
        "      currentTitle=corpusTxtList[i]\n",
        "      #paper\n",
        "      currentPaper=corpusTxtList[i+1]\n",
        "      #restarting BIbek\n",
        "      \n",
        "\n",
        "    #mid paper\n",
        "    if titleRow!=i and paperRow!=i and corpusTxtList[i]!='\\n':\n",
        "      #vector for storing begin and inside words\n",
        "      #print(corpusTxtList[i].split('\\t')[-1])\n",
        "      SematicTypeAppearence.append(normalizeSemanticType(corpusTxtList[i].split('\\t')[4]))\n",
        "\n",
        "    inPaper=True\n",
        "    #end of paper\n",
        "    if corpusTxtList[i]=='\\n':\n",
        "      titleRow=i+1\n",
        "      paperRow=i+2\n",
        "      inPaper=False\n",
        "      if paperNumber in trngTxtList:\n",
        "        paperKont+=1\n",
        "\n",
        "semanticDictionary=Counter(SematicTypeAppearence)\n",
        "#collections.OrderedDict(sorted(semanticDictionary.values()))\n",
        "dictionary=sorted(semanticDictionary.items(), key=operator.itemgetter(1))\n",
        "dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('T021', 2),\n",
              " ('T127', 2),\n",
              " ('T095', 19),\n",
              " ('T010', 20),\n",
              " ('T203', 20),\n",
              " ('T194', 25),\n",
              " ('T011', 27),\n",
              " ('T094', 36),\n",
              " ('T192', 52),\n",
              " ('T125', 54),\n",
              " ('T014', 58),\n",
              " ('T200', 61),\n",
              " ('T085', 61),\n",
              " ('T072', 79),\n",
              " ('T171', 99),\n",
              " ('T089', 102),\n",
              " ('T071', 109),\n",
              " ('T017', 134),\n",
              " ('T102', 154),\n",
              " ('T195', 180),\n",
              " ('T064', 188),\n",
              " ('T020', 189),\n",
              " ('T069', 190),\n",
              " ('T018', 267),\n",
              " ('T190', 271),\n",
              " ('T013', 276),\n",
              " ('T066', 279),\n",
              " ('T050', 288),\n",
              " ('T103', 289),\n",
              " ('T051', 292),\n",
              " ('T068', 295),\n",
              " ('T092', 300),\n",
              " ('T120', 311),\n",
              " ('T019', 347),\n",
              " ('T075', 365),\n",
              " ('T087', 365),\n",
              " ('T129', 365),\n",
              " ('T197', 368),\n",
              " ('T008', 429),\n",
              " ('T053', 447),\n",
              " ('T012', 465),\n",
              " ('T022', 517),\n",
              " ('T196', 525),\n",
              " ('T065', 554),\n",
              " ('T090', 565),\n",
              " ('T130', 579),\n",
              " ('T131', 581),\n",
              " ('T049', 594),\n",
              " ('T030', 602),\n",
              " ('T093', 604),\n",
              " ('T004', 612),\n",
              " ('T086', 624),\n",
              " ('T016', 732),\n",
              " ('T104', 799),\n",
              " ('T099', 816),\n",
              " ('T056', 863),\n",
              " ('T034', 874),\n",
              " ('T057', 891),\n",
              " ('T063', 923),\n",
              " ('T091', 938),\n",
              " ('T038', 946),\n",
              " ('T054', 982),\n",
              " ('T055', 1012),\n",
              " ('T001', 1038),\n",
              " ('T096', 1046),\n",
              " ('T005', 1105),\n",
              " ('T122', 1112),\n",
              " ('T029', 1151),\n",
              " ('T123', 1196),\n",
              " ('T031', 1258),\n",
              " ('T039', 1311),\n",
              " ('T168', 1368),\n",
              " ('T185', 1393),\n",
              " ('T024', 1582),\n",
              " ('T026', 1594),\n",
              " ('T042', 1699),\n",
              " ('T167', 1769),\n",
              " ('T204', 1792),\n",
              " ('T097', 1856),\n",
              " ('T184', 1886),\n",
              " ('T037', 1895),\n",
              " ('T002', 1924),\n",
              " ('T201', 2027),\n",
              " ('T067', 2034),\n",
              " ('T007', 2051),\n",
              " ('T100', 2157),\n",
              " ('T048', 2284),\n",
              " ('T015', 2299),\n",
              " ('T074', 2386),\n",
              " ('', 2473),\n",
              " ('T114', 2561),\n",
              " ('T040', 2713),\n",
              " ('T070', 2827),\n",
              " ('T077', 2997),\n",
              " ('T083', 3007),\n",
              " ('T043', 3089),\n",
              " ('T041', 3235),\n",
              " ('T045', 3438),\n",
              " ('T044', 3471),\n",
              " ('T073', 3615),\n",
              " ('T032', 3732),\n",
              " ('T121', 3951),\n",
              " ('T046', 3962),\n",
              " ('T060', 4083),\n",
              " ('T028', 4543),\n",
              " ('T191', 4734),\n",
              " ('T059', 5107),\n",
              " ('T025', 5536),\n",
              " ('T058', 5681),\n",
              " ('T101', 6300),\n",
              " ('T023', 6310),\n",
              " ('T098', 6319),\n",
              " ('T052', 7253),\n",
              " ('T082', 7576),\n",
              " ('T062', 8828),\n",
              " ('T170', 9208),\n",
              " ('T078', 9347),\n",
              " ('T079', 10169),\n",
              " ('T109', 10281),\n",
              " ('T061', 11429),\n",
              " ('T047', 11709),\n",
              " ('T116', 14970),\n",
              " ('T033', 15675),\n",
              " ('T081', 19995),\n",
              " ('T169', 23661),\n",
              " ('T080', 31485)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SMGyeUAibIp",
        "outputId": "100002a9-b4c7-4744-afde-011bd5fabc30"
      },
      "source": [
        "n_items=[]\n",
        "n_number_of_ner_tags=15\n",
        "\n",
        "for i in reversed(range(len(dictionary)-n_number_of_ner_tags, len(dictionary))):\n",
        "  n_items.append(dictionary[i][0])\n",
        "\n",
        "n_items"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T080',\n",
              " 'T169',\n",
              " 'T081',\n",
              " 'T033',\n",
              " 'T116',\n",
              " 'T047',\n",
              " 'T061',\n",
              " 'T109',\n",
              " 'T079',\n",
              " 'T078',\n",
              " 'T170',\n",
              " 'T062',\n",
              " 'T082',\n",
              " 'T052',\n",
              " 'T098']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8tREbdLWj-0"
      },
      "source": [
        "CREATING FUNCTIONS FOR CREATING TSV FILES FOR TRAINING DEV AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lizzgEu_rmD1"
      },
      "source": [
        "import csv\n",
        "\n",
        "def textToTokens(title, paper):\n",
        "  title=title.split()\n",
        "  paper=paper.split()\n",
        "  title[0]=title[0].split('|t|')[1]\n",
        "  paper[0]=paper[0].split('|a|')[1]\n",
        "  text = title+paper\n",
        "  #print(text)\n",
        "  return text\n",
        "  \n",
        "def textAndBItoTSV(tsv_writer, text, BI):#\n",
        "\n",
        "  textCount=0\n",
        "  for i in BI:\n",
        "    word = i[0].split()\n",
        "    SEMTYPE=i[1].rstrip(\"\\n\")\n",
        "    #print(word[0])\n",
        "    #print(text[textCount])\n",
        "    for w in range(0,len(word)):\n",
        "      #print(\"for\")\n",
        "      if textCount<len(text): #puede que sea <=\n",
        "\n",
        "        #normalize strings\n",
        "        #print(\"word[w]\"+str(word[w]))\n",
        "        word[w], isEndOrComma=normalizeString(word[w])\n",
        "        #print(\"text[textCount\"+str(textCount)+\"]\"+str(text[textCount]))\n",
        "        text[textCount], isEndOrComma=normalizeString(text[textCount])\n",
        "\n",
        "        if word[w]==text[textCount]:\n",
        "          if w==0:\n",
        "            #print(word[w])\n",
        "            #print(text[textCount])\n",
        "            if word[w]!=\"\" and text[textCount]!=\"\":\n",
        "              if SEMTYPE in n_items:\n",
        "                tsv_writer.writerow([text[textCount].lower(), 'B-'+SEMTYPE ])\n",
        "              else:\n",
        "                tsv_writer.writerow([text[textCount].lower(), 'O' ])\n",
        "              #print(\"B\")\n",
        "              #print(\"word: \"+str(word[w]))\n",
        "              #print(\"textCount: \"+str(text[textCount]))\n",
        "              if isEndOrComma==\"END\":\n",
        "                tsv_writer.writerow([\".\", 'O'])\n",
        "                tsv_writer.writerow([\"\", ''])\n",
        "              if isEndOrComma==\"COMMA\":\n",
        "                tsv_writer.writerow([\",\", 'O'])\n",
        "\n",
        "            textCount+=1\n",
        "            \n",
        "          else:\n",
        "            if word[w]!=\"\" and text[textCount]!=\"\":\n",
        "              if SEMTYPE in n_items:\n",
        "                tsv_writer.writerow([text[textCount].lower(), 'I-'+SEMTYPE ])\n",
        "              else:\n",
        "                tsv_writer.writerow([text[textCount].lower(), 'O' ])\n",
        "              #print(\"I\")\n",
        "              #print(\"word: \"+str(word[w]))\n",
        "              #print(\"textCount: \"+str(text[textCount]))\n",
        "              if isEndOrComma==\"END\":\n",
        "                tsv_writer.writerow([\".\", 'O'])\n",
        "                tsv_writer.writerow([\"\", ''])\n",
        "              if isEndOrComma==\"COMMA\":\n",
        "                tsv_writer.writerow([\",\", 'O'])\n",
        "            textCount+=1\n",
        "            \n",
        "        else:\n",
        "          justEntered=True\n",
        "          while textCount<len(text)-1:\n",
        "            #print(\"while\")\n",
        "            if justEntered==False:\n",
        "              #print(\"word[w]\"+str(word[w]))\n",
        "              word[w], isEndOrComma=normalizeString(word[w])\n",
        "              #print(normalizeString(text[textCount]))\n",
        "              #print(\"text[textCount\"+str(textCount)+\"]\"+str(text[textCount]))\n",
        "              text[textCount], isEndOrComma=normalizeString(text[textCount])\n",
        "            justEntered=False\n",
        "            if word[w]!=text[textCount]:\n",
        "              if word[w]!=\"\" and text[textCount]!=\"\":\n",
        "                tsv_writer.writerow([text[textCount].lower(), 'O'])\n",
        "                if isEndOrComma==\"END\":\n",
        "                  tsv_writer.writerow([\".\", 'O'])\n",
        "                  tsv_writer.writerow([\"\", ''])\n",
        "                if isEndOrComma==\"COMMA\":\n",
        "                  tsv_writer.writerow([\",\", 'O'])\n",
        "\n",
        "              #print(\"O\")\n",
        "              #print(\"word: \"+str(word[w]))\n",
        "              #print(\"textCount: \"+str(text[textCount]))\n",
        "              textCount+=1\n",
        "            else:\n",
        "              if w==0:\n",
        "                #print(\"B\")\n",
        "                #print(\"word: \"+str(word[w]))\n",
        "                #print(\"textCount: \"+str(text[textCount]))\n",
        "                if word[w]!=\"\" and text[textCount]!=\"\":\n",
        "                  if SEMTYPE in n_items:\n",
        "                    tsv_writer.writerow([text[textCount].lower(), 'B-'+SEMTYPE ])\n",
        "                  else:\n",
        "                    tsv_writer.writerow([text[textCount].lower(), 'O' ])\n",
        "                  if isEndOrComma==\"END\":\n",
        "                    tsv_writer.writerow([\".\", 'O'])\n",
        "                    tsv_writer.writerow([\"\", ''])\n",
        "                  if isEndOrComma==\"COMMA\":\n",
        "                    tsv_writer.writerow([\",\", 'O'])\n",
        "                textCount+=1\n",
        "                break\n",
        "              else:\n",
        "                #print(\"I\")\n",
        "                #print(\"word: \"+str(word[w]))\n",
        "                #print(\"textCount: \"+str(text[textCount]))\n",
        "                if word[w]!=\"\" and text[textCount]!=\"\":\n",
        "                  if SEMTYPE in n_items:\n",
        "                    tsv_writer.writerow([text[textCount].lower(), 'I-'+SEMTYPE ])\n",
        "                  else:\n",
        "                    tsv_writer.writerow([text[textCount].lower(), 'O' ])\n",
        "                  if isEndOrComma==\"END\":\n",
        "                    tsv_writer.writerow([\".\", 'O'])\n",
        "                    tsv_writer.writerow([\"\", ''])\n",
        "                  if isEndOrComma==\"COMMA\":\n",
        "                    tsv_writer.writerow([\",\", 'O'])\n",
        "                textCount+=1\n",
        "                break\n",
        "         \n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keeY43VSBSzw"
      },
      "source": [
        "CREATING TRAINING SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d38I3Bpd0zU"
      },
      "source": [
        "inPaper=False\n",
        "titleRow=0\n",
        "paperRow=1\n",
        "BIbek=[]\n",
        "\n",
        "#COMMENT NEXT TWO LINES FOR WHOLE CORPUS\n",
        "#maxAmountOfPapers=100\n",
        "paperKont=0\n",
        "\n",
        "with open('/content/drive/MyDrive/medNer/medMentions-train-SemType15-version.tsv', 'wt') as out_file:\n",
        "  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "\n",
        "  for i in range(0,len(corpusTxtList)):\n",
        "    \n",
        "    #beginning of paper\n",
        "    if(inPaper==False):\n",
        "      #number of paper\n",
        "      paperNumber=corpusTxtList[i+2].split('\\t')[0]\n",
        "      #title\n",
        "      currentTitle=corpusTxtList[i]\n",
        "      #paper\n",
        "      currentPaper=corpusTxtList[i+1]\n",
        "      #restarting BIbek\n",
        "      \n",
        "\n",
        "    #mid paper\n",
        "    if titleRow!=i and paperRow!=i and corpusTxtList[i]!='\\n':\n",
        "      #vector for storing begin and inside words\n",
        "      #print(corpusTxtList[i].split('\\t')[-1])\n",
        "      elemOfBek=[corpusTxtList[i].split('\\t')[3],normalizeSemanticType(corpusTxtList[i].split('\\t')[4])]\n",
        "      BIbek.append(elemOfBek)\n",
        "    inPaper=True\n",
        "    #end of paper\n",
        "    if corpusTxtList[i]=='\\n':\n",
        "      titleRow=i+1\n",
        "      paperRow=i+2\n",
        "      inPaper=False\n",
        "      if paperNumber in trngTxtList:\n",
        "        #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "        #if paperKont<maxAmountOfPapers:\n",
        "        textAndBItoTSV(tsv_writer, textToTokens(currentTitle, currentPaper),BIbek)\n",
        "          #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "          #paperKont+=1\n",
        "      BIbek=[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwioBaAuDCOR"
      },
      "source": [
        "CREATING TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Qryto1C4AH"
      },
      "source": [
        "inPaper=False\n",
        "titleRow=0\n",
        "paperRow=1\n",
        "BIbek=[]\n",
        "\n",
        "#COMMENT NEXT TWO LINES FOR WHOLE CORPUS\n",
        "#maxAmountOfPapers=40\n",
        "paperKont=0\n",
        "\n",
        "with open('/content/drive/MyDrive/medNer/medMentions-test-SemType15-version.tsv', 'wt') as out_file:\n",
        "  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "\n",
        "  for i in range(0,len(corpusTxtList)):\n",
        "    \n",
        "    #beginning of paper\n",
        "    if(inPaper==False):\n",
        "      #number of paper\n",
        "      paperNumber=corpusTxtList[i+2].split('\\t')[0]\n",
        "      #title\n",
        "      currentTitle=corpusTxtList[i]\n",
        "      #paper\n",
        "      currentPaper=corpusTxtList[i+1]\n",
        "      #restarting BIbek\n",
        "      \n",
        "\n",
        "    #mid paper\n",
        "    if titleRow!=i and paperRow!=i and corpusTxtList[i]!='\\n':\n",
        "      #vector for storing begin and inside words\n",
        "      elemOfBek=[corpusTxtList[i].split('\\t')[3],normalizeSemanticType(corpusTxtList[i].split('\\t')[4])]\n",
        "      BIbek.append(elemOfBek)\n",
        "    inPaper=True\n",
        "    #end of paper\n",
        "    if corpusTxtList[i]=='\\n':\n",
        "      titleRow=i+1\n",
        "      paperRow=i+2\n",
        "      inPaper=False\n",
        "      if paperNumber in testTxtList:\n",
        "        #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "        #if paperKont<maxAmountOfPapers:\n",
        "        textAndBItoTSV(tsv_writer, textToTokens(currentTitle, currentPaper),BIbek)\n",
        "          #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "          #paperKont+=1\n",
        "      BIbek=[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2ErFC0DKfm"
      },
      "source": [
        "CREATING DEV SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjoX586NDILu"
      },
      "source": [
        "inPaper=False\n",
        "titleRow=0\n",
        "paperRow=1\n",
        "BIbek=[]\n",
        "\n",
        "#COMMENT NEXT TWO LINES FOR WHOLE CORPUS\n",
        "#maxAmountOfPapers=40\n",
        "paperKont=0\n",
        "\n",
        "with open('/content/drive/MyDrive/medNer/medMentions-dev-SemType15-version.tsv', 'wt') as out_file:\n",
        "  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "\n",
        "  for i in range(0,len(corpusTxtList)):\n",
        "    \n",
        "    #beginning of paper\n",
        "    if(inPaper==False):\n",
        "      #number of paper\n",
        "      paperNumber=corpusTxtList[i+2].split('\\t')[0]\n",
        "      #title\n",
        "      currentTitle=corpusTxtList[i]\n",
        "      #paper\n",
        "      currentPaper=corpusTxtList[i+1]\n",
        "      #restarting BIbek\n",
        "      \n",
        "\n",
        "    #mid paper\n",
        "    if titleRow!=i and paperRow!=i and corpusTxtList[i]!='\\n':\n",
        "      #vector for storing begin and inside words\n",
        "      elemOfBek=[corpusTxtList[i].split('\\t')[3],normalizeSemanticType(corpusTxtList[i].split('\\t')[4])]\n",
        "      BIbek.append(elemOfBek)\n",
        "    inPaper=True\n",
        "    #end of paper\n",
        "    if corpusTxtList[i]=='\\n':\n",
        "      titleRow=i+1\n",
        "      paperRow=i+2\n",
        "      inPaper=False\n",
        "      if paperNumber in devTxtList:\n",
        "        #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "        #if paperKont<maxAmountOfPapers:\n",
        "          textAndBItoTSV(tsv_writer, textToTokens(currentTitle, currentPaper),BIbek)\n",
        "          #COMMENT NEXT LINE FOR WHOLE CORPUS\n",
        "          #paperKont+=1\n",
        "\n",
        "      BIbek=[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvqXkJuWshB"
      },
      "source": [
        "###YOU CAN TEST THE TRAINING PART IF YOU ARE WILLING TO TRAIN AN ALREADY EXISTING MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QkUODUWOm5Z"
      },
      "source": [
        "###TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS45OK6FOpnZ"
      },
      "source": [
        "Flair "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gUQIAesDPV2",
        "outputId": "76346fd5-9893-422b-9164-ab56c2460c59"
      },
      "source": [
        "pip install --upgrade flair"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.8MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 18.0MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Collecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 27.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Collecting requests<3.0.0,>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 25.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=03c693020b812a8fa0733bdb1f1c5f43d3d8124ac05782d306ed6957d81e38d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: mpld3, segtok, sqlitedict, ftfy, langdetect, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=6589c10b69d206c82ca41bc309b167ba358e60921781e29b1d11ee6132e1e4e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=759142eaa75473afcfac1790054b51a8755796e9067838ae8098f18ea07471f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=0fb4b593c368993a941aeaa4f76062c0faa28f1b34dd9f87ef20ea90af0bcedc\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=18f8a438f303e504da5a23fe15f3063abcd16d75a421f0a2321497a57f399279\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=23255a94bf6ca6b365e559a37f6c242fb7ddcfad692a4f02b0b836035a23a53e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=2abc4e61d67caf100892aad6f125783ac87019f6d6c0a124a57ce5b9677564dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built mpld3 segtok sqlitedict ftfy langdetect overrides\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, mpld3, segtok, requests, overrides, konoha, sqlitedict, janome, ftfy, huggingface-hub, bpemb, gdown, deprecated, langdetect, torch, tokenizers, sacremoses, transformers, flair\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.1 gdown-3.12.2 huggingface-hub-0.0.8 janome-0.4.1 konoha-4.6.4 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 requests-2.25.1 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.2 torch-1.7.1 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg8i2W3bO-kK"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus, NCBI_DISEASE\n",
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings#, TokenEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOPOT2KDPDrZ",
        "outputId": "dd905b3b-3c44-484a-fbe0-1faaed66cd2c"
      },
      "source": [
        "# define columns\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = '/content/drive/MyDrive/medNer'\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='medMentions-train-SemType10-version.tsv',\n",
        "                              test_file='medMentions-test-SemType10-version.tsv',\n",
        "                              dev_file='medMentions-dev-SemType10-version.tsv')\n",
        "\n",
        "# TODO obtain and print corpus statistics (output below obtained with the BASQUE NER corpus)\n",
        "print(corpus.obtain_statistics())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:44:26,801 Reading data from /content/drive/MyDrive/medNer\n",
            "2021-05-11 08:44:26,809 Train: /content/drive/MyDrive/medNer/medMentions-train-SemType10-version.tsv\n",
            "2021-05-11 08:44:26,812 Dev: /content/drive/MyDrive/medNer/medMentions-dev-SemType10-version.tsv\n",
            "2021-05-11 08:44:26,816 Test: /content/drive/MyDrive/medNer/medMentions-test-SemType10-version.tsv\n",
            "{\n",
            "    \"TRAIN\": {\n",
            "        \"dataset\": \"TRAIN\",\n",
            "        \"total_number_of_documents\": 25094,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 654178,\n",
            "            \"min\": 2,\n",
            "            \"max\": 173,\n",
            "            \"avg\": 26.069100183310752\n",
            "        }\n",
            "    },\n",
            "    \"TEST\": {\n",
            "        \"dataset\": \"TEST\",\n",
            "        \"total_number_of_documents\": 8312,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 219149,\n",
            "            \"min\": 2,\n",
            "            \"max\": 142,\n",
            "            \"avg\": 26.365375360923966\n",
            "        }\n",
            "    },\n",
            "    \"DEV\": {\n",
            "        \"dataset\": \"DEV\",\n",
            "        \"total_number_of_documents\": 8284,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 220293,\n",
            "            \"min\": 2,\n",
            "            \"max\": 158,\n",
            "            \"avg\": 26.592588121680347\n",
            "        }\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPOy9UqjPltR",
        "outputId": "ec36cb58-0179-4045-d9d5-c5d70a6a42c9"
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'<unk>', b'O', b'B-T116', b'B-T047', b'I-T047', b'B-T079', b'B-T169', b'I-T169', b'B-T033', b'I-T033', b'B-T081', b'I-T116', b'B-T109', b'B-T080', b'I-T109', b'B-T078', b'B-T061', b'I-T061', b'I-T080', b'I-T081', b'I-T078', b'I-T079', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVWpihB1Ww2Z"
      },
      "source": [
        "You can choose the embeddings. The ideal is the following combination:FlairEmbeddings('pubmed-forward'), FlairEmbeddings('pubmed-backward'), WordEmbeddings(\"pubmed\"). But due to memory issues just WordEmbeddings(\"pubmed\") is the best choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHkfSkqRP9Zh",
        "outputId": "3da553e0-1926-40ca-c665-aa25657eb021"
      },
      "source": [
        "embeddings : StackedEmbeddings = StackedEmbeddings([\n",
        "                                        #WordEmbeddings('glove'),\n",
        "                                        #FlairEmbeddings('news-forward'),\n",
        "                                        #FlairEmbeddings('news-backward')\n",
        "                                        #FlairEmbeddings('pubmed-forward')\n",
        "                                        #FlairEmbeddings('pubmed-backward')\n",
        "                                        WordEmbeddings(\"pubmed\"),\n",
        "\n",
        "                                        # flair embeddings trained on PubMed and PMC\n",
        "                                        #FlairEmbeddings(\"pubmed-forward\"),\n",
        "                                        #FlairEmbeddings(\"pubmed-backward\"),\n",
        "                                       ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:44:49,403 https://flair.informatik.hu-berlin.de/resources/embeddings/token/pubmed_pmc_wiki_sg_1M.gensim.vectors.npy not found in cache, downloading to /tmp/tmpq2cdzrtn\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800000128/800000128 [00:56<00:00, 14231511.82B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:45:46,052 copying /tmp/tmpq2cdzrtn to cache at /root/.flair/embeddings/pubmed_pmc_wiki_sg_1M.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:45:48,947 removing temp file /tmp/tmpq2cdzrtn\n",
            "2021-05-11 08:45:49,622 https://flair.informatik.hu-berlin.de/resources/embeddings/token/pubmed_pmc_wiki_sg_1M.gensim not found in cache, downloading to /tmp/tmpymy3_jwj\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 53979687/53979687 [00:02<00:00, 18236691.92B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:45:52,971 copying /tmp/tmpymy3_jwj to cache at /root/.flair/embeddings/pubmed_pmc_wiki_sg_1M.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:45:53,033 removing temp file /tmp/tmpymy3_jwj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHhfke98g0Gu"
      },
      "source": [
        "#from gensim.models import KeyedVectors\n",
        "#word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "#x = word2vec.word_vec(\"test\")\n",
        "#EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxOcl_JiQ5e_",
        "outputId": "8001c99d-63fc-4796-d12b-8487bfa43d6a"
      },
      "source": [
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                       embeddings=embeddings,\n",
        "                                       tag_dictionary=tag_dictionary,\n",
        "                                       tag_type=tag_type,\n",
        "                                       use_crf=True)\n",
        "print(tagger)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('pubmed')\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (rnn): LSTM(200, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=24, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ckA2WKRLhX",
        "outputId": "3b4f151f-631c-4e0c-fceb-beb17d902dfe"
      },
      "source": [
        "from flair.trainers import ModelTrainer\n",
        "trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "trainer.train('/content/drive/MyDrive/medNer',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=16,\n",
        "              max_epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-11 08:45:57,634 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,636 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('pubmed')\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (rnn): LSTM(200, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=24, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-05-11 08:45:57,638 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,640 Corpus: \"Corpus: 25094 train + 8284 dev + 8312 test sentences\"\n",
            "2021-05-11 08:45:57,644 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,647 Parameters:\n",
            "2021-05-11 08:45:57,649  - learning_rate: \"0.1\"\n",
            "2021-05-11 08:45:57,653  - mini_batch_size: \"16\"\n",
            "2021-05-11 08:45:57,655  - patience: \"3\"\n",
            "2021-05-11 08:45:57,658  - anneal_factor: \"0.5\"\n",
            "2021-05-11 08:45:57,672  - max_epochs: \"30\"\n",
            "2021-05-11 08:45:57,675  - shuffle: \"True\"\n",
            "2021-05-11 08:45:57,678  - train_with_dev: \"False\"\n",
            "2021-05-11 08:45:57,680  - batch_growth_annealing: \"False\"\n",
            "2021-05-11 08:45:57,684 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,688 Model training base path: \"/content/drive/MyDrive/medNer\"\n",
            "2021-05-11 08:45:57,691 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,694 Device: cpu\n",
            "2021-05-11 08:45:57,699 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:45:57,703 Embeddings storage mode: cpu\n",
            "2021-05-11 08:45:58,079 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:46:55,910 epoch 1 - iter 156/1569 - loss 25.38933201 - samples/sec: 43.17 - lr: 0.100000\n",
            "2021-05-11 08:47:48,415 epoch 1 - iter 312/1569 - loss 22.40585790 - samples/sec: 47.55 - lr: 0.100000\n",
            "2021-05-11 08:48:43,811 epoch 1 - iter 468/1569 - loss 20.65030688 - samples/sec: 45.07 - lr: 0.100000\n",
            "2021-05-11 08:49:44,779 epoch 1 - iter 624/1569 - loss 19.70083093 - samples/sec: 40.96 - lr: 0.100000\n",
            "2021-05-11 08:50:52,630 epoch 1 - iter 780/1569 - loss 19.02334475 - samples/sec: 36.80 - lr: 0.100000\n",
            "2021-05-11 08:51:59,538 epoch 1 - iter 936/1569 - loss 18.48984980 - samples/sec: 37.32 - lr: 0.100000\n",
            "2021-05-11 08:53:06,363 epoch 1 - iter 1092/1569 - loss 18.00333850 - samples/sec: 37.36 - lr: 0.100000\n",
            "2021-05-11 08:54:06,421 epoch 1 - iter 1248/1569 - loss 17.65295526 - samples/sec: 41.57 - lr: 0.100000\n",
            "2021-05-11 08:55:11,074 epoch 1 - iter 1404/1569 - loss 17.42218714 - samples/sec: 38.62 - lr: 0.100000\n",
            "2021-05-11 08:56:07,750 epoch 1 - iter 1560/1569 - loss 17.21470071 - samples/sec: 44.05 - lr: 0.100000\n",
            "2021-05-11 08:56:11,128 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 08:56:11,132 EPOCH 1 done: loss 17.1923 - lr 0.1000000\n",
            "2021-05-11 08:56:50,672 DEV : loss 13.836381912231445 - score 0.2031\n",
            "2021-05-11 08:56:51,223 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-05-11 08:57:17,139 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 09:04:25,874 epoch 2 - iter 156/1569 - loss 14.95376536 - samples/sec: 5.82 - lr: 0.100000\n",
            "2021-05-11 09:10:55,969 epoch 2 - iter 312/1569 - loss 14.58119568 - samples/sec: 6.40 - lr: 0.100000\n",
            "2021-05-11 09:17:51,804 epoch 2 - iter 468/1569 - loss 14.50276539 - samples/sec: 6.00 - lr: 0.100000\n",
            "2021-05-11 09:24:24,800 epoch 2 - iter 624/1569 - loss 14.26696123 - samples/sec: 6.35 - lr: 0.100000\n",
            "2021-05-11 09:31:05,301 epoch 2 - iter 780/1569 - loss 14.22919556 - samples/sec: 6.23 - lr: 0.100000\n",
            "2021-05-11 09:37:46,102 epoch 2 - iter 936/1569 - loss 14.15426207 - samples/sec: 6.23 - lr: 0.100000\n",
            "2021-05-11 09:44:36,352 epoch 2 - iter 1092/1569 - loss 14.08556536 - samples/sec: 6.08 - lr: 0.100000\n",
            "2021-05-11 09:50:08,631 epoch 2 - iter 1248/1569 - loss 14.05076780 - samples/sec: 7.51 - lr: 0.100000\n",
            "2021-05-11 09:55:49,810 epoch 2 - iter 1404/1569 - loss 14.02189132 - samples/sec: 7.32 - lr: 0.100000\n",
            "2021-05-11 10:01:36,769 epoch 2 - iter 1560/1569 - loss 13.99183114 - samples/sec: 7.19 - lr: 0.100000\n",
            "2021-05-11 10:01:59,791 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 10:01:59,794 EPOCH 2 done: loss 13.9914 - lr 0.1000000\n",
            "2021-05-11 10:03:32,628 DEV : loss 11.961236000061035 - score 0.2569\n",
            "2021-05-11 10:03:33,246 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-05-11 10:03:53,760 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 10:12:11,526 epoch 3 - iter 156/1569 - loss 13.31857079 - samples/sec: 5.01 - lr: 0.100000\n",
            "2021-05-11 10:19:40,306 epoch 3 - iter 312/1569 - loss 13.31942023 - samples/sec: 5.56 - lr: 0.100000\n",
            "2021-05-11 10:27:04,618 epoch 3 - iter 468/1569 - loss 13.43147102 - samples/sec: 5.62 - lr: 0.100000\n",
            "2021-05-11 10:34:07,216 epoch 3 - iter 624/1569 - loss 13.34784211 - samples/sec: 5.91 - lr: 0.100000\n",
            "2021-05-11 10:41:38,084 epoch 3 - iter 780/1569 - loss 13.39559237 - samples/sec: 5.54 - lr: 0.100000\n",
            "2021-05-11 10:49:30,278 epoch 3 - iter 936/1569 - loss 13.34405017 - samples/sec: 5.29 - lr: 0.100000\n",
            "2021-05-11 10:57:01,477 epoch 3 - iter 1092/1569 - loss 13.32300221 - samples/sec: 5.53 - lr: 0.100000\n",
            "2021-05-11 11:04:39,269 epoch 3 - iter 1248/1569 - loss 13.27532622 - samples/sec: 5.45 - lr: 0.100000\n",
            "2021-05-11 11:12:46,095 epoch 3 - iter 1404/1569 - loss 13.26840093 - samples/sec: 5.13 - lr: 0.100000\n",
            "2021-05-11 11:20:51,232 epoch 3 - iter 1560/1569 - loss 13.24161421 - samples/sec: 5.15 - lr: 0.100000\n",
            "2021-05-11 11:21:16,835 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 11:21:16,838 EPOCH 3 done: loss 13.2329 - lr 0.1000000\n",
            "2021-05-11 11:23:09,559 DEV : loss 11.517888069152832 - score 0.32\n",
            "2021-05-11 11:23:10,142 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-05-11 11:23:28,209 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 11:32:58,696 epoch 4 - iter 156/1569 - loss 12.78270237 - samples/sec: 4.38 - lr: 0.100000\n",
            "2021-05-11 11:42:28,529 epoch 4 - iter 312/1569 - loss 12.83366708 - samples/sec: 4.38 - lr: 0.100000\n",
            "2021-05-11 11:51:54,335 epoch 4 - iter 468/1569 - loss 12.80031986 - samples/sec: 4.41 - lr: 0.100000\n",
            "2021-05-11 12:01:35,103 epoch 4 - iter 624/1569 - loss 12.78294199 - samples/sec: 4.30 - lr: 0.100000\n",
            "2021-05-11 12:11:12,649 epoch 4 - iter 780/1569 - loss 12.73269627 - samples/sec: 4.32 - lr: 0.100000\n",
            "2021-05-11 12:21:02,244 epoch 4 - iter 936/1569 - loss 12.72068503 - samples/sec: 4.23 - lr: 0.100000\n",
            "2021-05-11 12:30:35,412 epoch 4 - iter 1092/1569 - loss 12.74594513 - samples/sec: 4.35 - lr: 0.100000\n",
            "2021-05-11 12:40:07,815 epoch 4 - iter 1248/1569 - loss 12.70773511 - samples/sec: 4.36 - lr: 0.100000\n",
            "2021-05-11 12:49:28,027 epoch 4 - iter 1404/1569 - loss 12.68319327 - samples/sec: 4.46 - lr: 0.100000\n",
            "2021-05-11 12:58:49,835 epoch 4 - iter 1560/1569 - loss 12.71951039 - samples/sec: 4.44 - lr: 0.100000\n",
            "2021-05-11 12:59:18,360 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 12:59:18,368 EPOCH 4 done: loss 12.7247 - lr 0.1000000\n",
            "2021-05-11 13:01:18,953 DEV : loss 11.040719032287598 - score 0.2771\n",
            "2021-05-11 13:01:19,550 BAD EPOCHS (no improvement): 1\n",
            "2021-05-11 13:01:19,561 ----------------------------------------------------------------------------------------------------\n",
            "2021-05-11 13:10:26,023 epoch 5 - iter 156/1569 - loss 12.54970937 - samples/sec: 4.57 - lr: 0.100000\n",
            "2021-05-11 13:20:26,595 epoch 5 - iter 312/1569 - loss 12.64223361 - samples/sec: 4.16 - lr: 0.100000\n",
            "2021-05-11 13:30:08,768 epoch 5 - iter 468/1569 - loss 12.57315647 - samples/sec: 4.29 - lr: 0.100000\n",
            "2021-05-11 13:39:25,026 epoch 5 - iter 624/1569 - loss 12.56155756 - samples/sec: 4.49 - lr: 0.100000\n",
            "2021-05-11 13:49:30,707 epoch 5 - iter 780/1569 - loss 12.55115879 - samples/sec: 4.12 - lr: 0.100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfMUE4-CYPwd"
      },
      "source": [
        "LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVQcFwl1YJ9J",
        "outputId": "29b7be98-3176-4f08-e920-06808ad5d0d3"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "# load the trained model\n",
        "model = SequenceTagger.load('/content/drive/MyDrive/medNer/best-model.pt')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-12 11:47:03,350 loading file /content/drive/MyDrive/medNer/best-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZSPymuYbZf"
      },
      "source": [
        "RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC1hqeZoYW00",
        "outputId": "e70902cb-5636-46f6-8b5d-9488b3899626"
      },
      "source": [
        "from segtok.segmenter import split_single\n",
        "#read file\n",
        "Text=\"/content/drive/MyDrive/medNer/paper.txt\"\n",
        "with open(Text, \"r\") as file1:\n",
        "    FileasList = file1.readlines()\n",
        "TextStr= FileasList[0]\n",
        "\n",
        "#NER\n",
        "#2.Whole text predict\n",
        "TextStrAll=Sentence(TextStr)\n",
        "model.predict(TextStrAll)\n",
        "print(TextStrAll.to_tagged_string())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The purpose of this study was to evaluate the effect <B-T080> of femoral tunnel orientation , drilled through the accessory anteromedial ( AAM ) portal or the high AM portal in anatomic anterior cruciate ligament ( ACL ) reconstruction <B-T061> . In 16 cadaver knees , using o'clock method , centers of the ACL femoral footprint were drilled with an 8-mm reamer via an AAM portal ( eight knees ) or a high AM portal ( eight knees ) . Computed tomography ( CT ) scans were taken of each knee . Three-dimensional ( 3D ) models were constructed to identify the femoral tunnel orientation and to create femoral tunnel virtual cylinders for measuring tunnel angles and length <B-T081> . In two of the 16 specimens , we observed a posterior femoral cortex blowout ( PFCB ) when drilling through a high AM portal . When drilled through the high AM portal , the femoral tunnel length <B-T081> was significantly shorter than when using an AAM portal ( 30.3 ± 3.8 mm and 38.2 ± 3.1 mm , p < 0.001 ) . The femoral tunnel length <B-T081> was significantly shorter in the group <B-T078> with PFCB compared to the group <B-T078> with no PFCB ( 25.9 ± 0.6 mm and 35.5 ± 4.5 mm , p = 0.011 ) . The axial obliquity of the high AM portal was significantly higher than that of the AAM portal ( 52.2 ± 5.9 ° and 43.0 ± 2.3 ° , p = 0.003 ) . In anatomic <B-T061> ACL <I-T061> reconstruction <I-T061> , a mal-positioned <B-T061> AM <I-T061> portal <I-T061> can cause abnormal tunnel orientation , which may lead to mechanical failure during ACL <B-T061> reconstruction <I-T061> . Therefore , it is important to select accurate AM portal positioning , and possibly using an AAM portal by measuring an accurate <B-T080> position when drilling a femoral tunnel in anatomic <B-T061> ACL <I-T061> reconstruction <I-T061> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY3PbyNcaa-4"
      },
      "source": [
        "The real paper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCx7BKNYZ9SP"
      },
      "source": [
        "28366539|t|Influence of the different anteromedial portal on femoral tunnel orientation during anatomic ACL reconstruction\n",
        "28366539|a|The purpose of this study was to evaluate the effect of femoral tunnel orientation, drilled through the accessory anteromedial (AAM) portal or the high AM portal in anatomic anterior cruciate ligament (ACL) reconstruction. In 16 cadaver knees, using o'clock method, centers of the ACL femoral footprint were drilled with an 8-mm reamer via an AAM portal (eight knees) or a high AM portal (eight knees). Computed tomography (CT) scans were taken of each knee. Three-dimensional (3D) models were constructed to identify the femoral tunnel orientation and to create femoral tunnel virtual cylinders for measuring tunnel angles and length. In two of the 16 specimens, we observed a posterior femoral cortex blowout (PFCB) when drilling through a high AM portal. When drilled through the high AM portal, the femoral tunnel length was significantly shorter than when using an AAM portal (30.3 ± 3.8 mm and 38.2 ± 3.1 mm, p < 0.001). The femoral tunnel length was significantly shorter in the group with PFCB compared to the group with no PFCB (25.9 ± 0.6 mm and 35.5 ± 4.5 mm, p = 0.011). The axial obliquity of the high AM portal was significantly higher than that of the AAM portal (52.2 ± 5.9° and 43.0 ± 2.3°, p = 0.003). In anatomic ACL reconstruction, a mal-positioned AM portal can cause abnormal tunnel orientation, which may lead to mechanical failure during ACL reconstruction. Therefore, it is important to select accurate AM portal positioning, and possibly using an AAM portal by measuring an accurate position when drilling a femoral tunnel in anatomic ACL reconstruction.\n",
        "28366539\t17\t26\tdifferent\tT080\tC1705242\n",
        "28366539\t27\t46\tanteromedial portal\tT061\tC0185154\n",
        "28366539\t50\t64\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t65\t76\torientation\tT082\tC1704322\n",
        "28366539\t84\t92\tanatomic\tT080\tC0220784\n",
        "28366539\t93\t111\tACL reconstruction\tT061\tC0188185\n",
        "28366539\t145\t153\tevaluate\tT058\tC0220825\n",
        "28366539\t158\t164\teffect\tT080\tC1280500\n",
        "28366539\t168\t182\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t183\t194\torientation\tT082\tC1704322\n",
        "28366539\t196\t203\tdrilled\tT061\tC0337279\n",
        "28366539\t216\t251\taccessory anteromedial (AAM) portal\tT061\tC0185154\n",
        "28366539\t259\t273\thigh AM portal\tT061\tC0185154\n",
        "28366539\t277\t333\tanatomic anterior cruciate ligament (ACL) reconstruction\tT061\tC0188185\n",
        "28366539\t341\t348\tcadaver\tT017\tC0006629\n",
        "28366539\t349\t354\tknees\tT023\tC0022742\n",
        "28366539\t362\t376\to'clock method\tT061\tC1293156\n",
        "28366539\t378\t385\tcenters\tT082\tC0205099\n",
        "28366539\t393\t414\tACL femoral footprint\tT023\tC0078960\n",
        "28366539\t420\t427\tdrilled\tT061\tC0337279\n",
        "28366539\t441\t447\treamer\tT074\tC3853551\n",
        "28366539\t455\t465\tAAM portal\tT061\tC0185154\n",
        "28366539\t473\t478\tknees\tT023\tC0022742\n",
        "28366539\t485\t499\thigh AM portal\tT061\tC0087111\n",
        "28366539\t507\t512\tknees\tT023\tC0022742\n",
        "28366539\t515\t545\tComputed tomography (CT) scans\tT060\tC0040405\n",
        "28366539\t565\t569\tknee\tT023\tC0022742\n",
        "28366539\t571\t600\tThree-dimensional (3D) models\tT075\tC0026336\n",
        "28366539\t634\t648\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t649\t660\torientation\tT082\tC1704322\n",
        "28366539\t668\t674\tcreate\tT052\tC1706214\n",
        "28366539\t675\t707\tfemoral tunnel virtual cylinders\tT023\tC0015811\n",
        "28366539\t712\t721\tmeasuring\tT080\tC0444706\n",
        "28366539\t722\t735\ttunnel angles\tT030\tC0229984\n",
        "28366539\t740\t746\tlength\tT081\tC1444754\n",
        "28366539\t765\t774\tspecimens\tT167\tC0370003\n",
        "28366539\t779\t787\tobserved\tT169\tC1441672\n",
        "28366539\t790\t822\tposterior femoral cortex blowout\tT046\tC0021890\n",
        "28366539\t824\t828\tPFCB\tT046\tC0021890\n",
        "28366539\t835\t843\tdrilling\tT061\tC0337279\n",
        "28366539\t844\t851\tthrough\tT169\tC0332273\n",
        "28366539\t854\t868\thigh AM portal\tT061\tC0185154\n",
        "28366539\t875\t882\tdrilled\tT061\tC0337279\n",
        "28366539\t883\t890\tthrough\tT169\tC0332273\n",
        "28366539\t895\t909\thigh AM portal\tT061\tC0185154\n",
        "28366539\t915\t929\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t930\t936\tlength\tT081\tC1444754\n",
        "28366539\t941\t954\tsignificantly\tT078\tC0750502\n",
        "28366539\t955\t962\tshorter\tT081\tC1806781\n",
        "28366539\t982\t992\tAAM portal\tT061\tC0185154\n",
        "28366539\t1043\t1057\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t1058\t1064\tlength\tT081\tC1444754\n",
        "28366539\t1069\t1082\tsignificantly\tT078\tC0750502\n",
        "28366539\t1083\t1090\tshorter\tT081\tC1806781\n",
        "28366539\t1098\t1103\tgroup\tT078\tC0441833\n",
        "28366539\t1109\t1113\tPFCB\tT046\tC0021890\n",
        "28366539\t1114\t1122\tcompared\tT052\tC1707455\n",
        "28366539\t1130\t1135\tgroup\tT078\tC0441833\n",
        "28366539\t1141\t1148\tno PFCB\tT033\tC0243095\n",
        "28366539\t1199\t1214\taxial obliquity\tT082\tC0205315\n",
        "28366539\t1222\t1236\thigh AM portal\tT061\tC0185154\n",
        "28366539\t1241\t1254\tsignificantly\tT078\tC0750502\n",
        "28366539\t1255\t1261\thigher\tT080\tC0205250\n",
        "28366539\t1279\t1289\tAAM portal\tT061\tC0185154\n",
        "28366539\t1335\t1343\tanatomic\tT080\tC0220784\n",
        "28366539\t1344\t1362\tACL reconstruction\tT061\tC0188185\n",
        "28366539\t1366\t1380\tmal-positioned\tT082\tC0333042\n",
        "28366539\t1381\t1390\tAM portal\tT061\tC0185154\n",
        "28366539\t1401\t1409\tabnormal\tT033\tC0205161\n",
        "28366539\t1410\t1416\ttunnel\tT023\tC0015811\n",
        "28366539\t1417\t1428\torientation\tT082\tC1704322\n",
        "28366539\t1448\t1458\tmechanical\tT169\tC0443254\n",
        "28366539\t1474\t1492\tACL reconstruction\tT061\tC0188185\n",
        "28366539\t1531\t1539\taccurate\tT080\tC0443131\n",
        "28366539\t1540\t1549\tAM portal\tT061\tC0185154\n",
        "28366539\t1550\t1561\tpositioning\tT082\tC0733755\n",
        "28366539\t1585\t1595\tAAM portal\tT061\tC0185154\n",
        "28366539\t1599\t1608\tmeasuring\tT080\tC0444706\n",
        "28366539\t1612\t1620\taccurate\tT080\tC0443131\n",
        "28366539\t1621\t1629\tposition\tT082\tC0733755\n",
        "28366539\t1635\t1643\tdrilling\tT061\tC0337279\n",
        "28366539\t1646\t1660\tfemoral tunnel\tT023\tC0015811\n",
        "28366539\t1664\t1672\tanatomic\tT080\tC0220784\n",
        "28366539\t1673\t1691\tACL reconstruction\tT061\tC0188185"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}